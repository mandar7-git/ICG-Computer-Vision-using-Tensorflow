{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_200132",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Aim**  \n",
        "The motive of this assignment is to make predictions using **Linear Regression**. To make sure you truly understand how the underlying algorithm works, you are to implement it from scratch."
      ],
      "metadata": {
        "id": "RB2d1J1f1CF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating the dataset  \n",
        "Run the cell below to create the dataset. It further splits the available data into training and testing. Please do not edit this cell.\n"
      ],
      "metadata": {
        "id": "a_S80lf6H4Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate the data\n",
        "X, y = datasets.make_regression(n_samples=100, n_features=5, noise=20, random_state=4)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n"
      ],
      "metadata": {
        "id": "yX0zqXcHIQHP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the data  \n",
        "Use `matplotlib` to visualize the given data."
      ],
      "metadata": {
        "id": "Zj4rrRXGJBXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i in range(0,5):\n",
        "  plt.scatter(X[:,1],y)\n",
        "  plt.show\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "zxfi8dkBJOUi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "18f86128-4b6d-4622-bc8f-43f687e64645"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should be able to see the linear relations between `y` and the features in vector `X`."
      ],
      "metadata": {
        "id": "r7vndSBAJceF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent Review  \n",
        "1. ####  Cost function\n",
        "Define the `cost function` to measure the difference between predictions and target outputs. Here, we are working with first degree polynomial, so derivatives are easy to calculate. ( Linear function `y = wx +b` )  \n",
        "\n",
        "$$Error = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\overline{y}_i)^2 = \\frac{1}{N}\\sum_{i=1}^N (y_i - (x_iw+b))^2 $$  \n",
        "\n",
        "  where `N` is the number of samples  \n",
        "    \n",
        "\n",
        "\n",
        "2. #### Compute the derivative\n",
        "$$\\frac{\\delta Error}{\\delta w} = \\frac{2}{N}\\sum_{i=1}^N -x_i(y_i -(m  x_i +b ))  $$\n",
        "$$\\frac{\\delta Error}{\\delta b} = \\frac{2}{N}\\sum_{i=1}^N -(y_i -(m  x_i +b ))  $$\n",
        "3. <h4>Update current parameters</h4>\n",
        "$$ w:= w- learning\\_rate \\cdot \\frac{\\delta Error}{\\delta w}   $$ \n",
        "$$ b:= b- learning\\_rate \\cdot \\frac{\\delta Error}{\\delta b}   $$ \n",
        "4. <h4>Repeat until it fits good enough</h4>\n"
      ],
      "metadata": {
        "id": "b4I9Z3epNvBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition\n",
        "\n",
        "Complete the functions in the class below. Hints provided at appropriate places."
      ],
      "metadata": {
        "id": "kBtUcOVnJu-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegression:\n",
        "\n",
        "    # The __init__ is called when we make any object of our class. Here, you are to specify the default values for \n",
        "    # Learning Rate, Number of Iterations, Weights and Biases. It doesn't return anything.\n",
        "    # Hint: Google what a `self pointer` is and figure out how it can be used here.\n",
        "    def __init__(self, learning_rate=0.001, n_iters=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "       \n",
        "\n",
        "          # Uncomment this when you're done with this function\n",
        "\n",
        "\n",
        "    # The following function would be the heart of the model. This is where the training would happen. \n",
        "    # You're supposed to iterate and keep on updating the weights and biases according to the steps of Gradient Descent.\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "         # init parameters\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        # Gradient Descent code goes here\n",
        "        for _ in range(self.n_iters):\n",
        "            y_predicted = np.dot(X, self.weights) + self.bias\n",
        "            \n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "          # Uncomment this when you're done with this function\n",
        "        \n",
        "        \n",
        "    # This function will be called after our model has been trained and we are predicting on unseen data\n",
        "    # What is our prediction? Just return that\n",
        "    def predict(self, X):\n",
        "       y_fin = np.dot(X, self.weights) + self.bias\n",
        "       return y_fin\n",
        "\n",
        "\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "dGnFNPJx3I28"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing, Training & Predictions"
      ],
      "metadata": {
        "id": "EvyInkTKPn7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we make an object of our custom class.\n",
        "\n",
        "regressor = LinearRegression(learning_rate=0.01, n_iters=1000)# You may pass the custom parameters or let the default values take it ahead\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Call the fit method on the object to train (pass appropriate part of dataset)\n",
        "\n",
        "\n",
        "# Now, let's see our what our model predicts\n",
        "predictions = regressor.predict(X_test) # pass appropriate part of dataset"
      ],
      "metadata": {
        "id": "nvItUpAkHTiv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model   \n",
        "\n",
        "Return [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) & [R2 Score](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/coefficient-of-determination-r-squared.html#:~:text=%C2%AFy) from the functions below."
      ],
      "metadata": {
        "id": "tzK6cq8eRD4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "       # return the mean squared error\n",
        "        return np.mean((y_true - y_pred) ** 2) # Uncomment this when you're done with this function\n",
        "\n",
        "\n",
        "def r2_score(y_true, y_pred):\n",
        "      # return the r2 score\n",
        "      corr_matrix = np.corrcoef(y_true, y_pred)\n",
        "      corr = corr_matrix[0, 1]\n",
        "      return corr ** 2  # Uncomment this when you're done with this function\n",
        "          \n",
        "\n",
        "mse = mean_squared_error(y_test, predictions) # Pass appropriate parts of dataset\n",
        "print(\"MSE:\", mse)\n",
        "\n",
        "accu = r2_score(y_test, predictions) # Pass appropriate parts of dataset\n",
        "print(\"Accuracy:\", accu)"
      ],
      "metadata": {
        "id": "WqkrvDzcRF5m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ca01759-263e-47a4-dbaa-6f9cd07f1326"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 390.6026570894989\n",
            "Accuracy: 0.9637788781894968\n"
          ]
        }
      ]
    }
  ]
}