{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CV with TF: Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Aim**  \n",
        "The motive of this assignment is to make predictions using **Linear Regression**. To make sure you truly understand how the underlying algorithm works, you are to implement it from scratch."
      ],
      "metadata": {
        "id": "RB2d1J1f1CF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating the dataset  \n",
        "Run the cell below to create the dataset. It further splits the available data into training and testing. Please do not edit this cell.\n"
      ],
      "metadata": {
        "id": "a_S80lf6H4Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate the data\n",
        "X, y = datasets.make_regression(n_samples=100, n_features=5, noise=20, random_state=4)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
      ],
      "metadata": {
        "id": "yX0zqXcHIQHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the data  \n",
        "Use `matplotlib` to visualize the given data."
      ],
      "metadata": {
        "id": "Zj4rrRXGJBXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "corr=np.corrcoef(X_train,rowvar=False)\n",
        "plt.matshow(corr);\n",
        "\n",
        "\n",
        "\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "zxfi8dkBJOUi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "ea3b9540-2920-4958-f59a-740b3939b57c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should be able to see the linear relations between `y` and the features in vector `X`."
      ],
      "metadata": {
        "id": "r7vndSBAJceF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent Review  \n",
        "1. ####  Cost function\n",
        "Define the `cost function` to measure the difference between predictions and target outputs. Here, we are working with first degree polynomial, so derivatives are easy to calculate. ( Linear function `y = wx +b` )  \n",
        "\n",
        "$$Error = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\overline{y}_i)^2 = \\frac{1}{N}\\sum_{i=1}^N (y_i - (x_iw+b))^2 $$  \n",
        "\n",
        "  where `N` is the number of samples  \n",
        "    \n",
        "\n",
        "\n",
        "2. #### Compute the derivative\n",
        "$$\\frac{\\delta Error}{\\delta w} = \\frac{2}{N}\\sum_{i=1}^N -x_i(y_i -(m  x_i +b ))  $$\n",
        "$$\\frac{\\delta Error}{\\delta b} = \\frac{2}{N}\\sum_{i=1}^N -(y_i -(m  x_i +b ))  $$\n",
        "3. <h4>Update current parameters</h4>\n",
        "$$ w:= w- learning\\_rate \\cdot \\frac{\\delta Error}{\\delta w}   $$ \n",
        "$$ b:= b- learning\\_rate \\cdot \\frac{\\delta Error}{\\delta b}   $$ \n",
        "4. <h4>Repeat until it fits good enough</h4>\n"
      ],
      "metadata": {
        "id": "b4I9Z3epNvBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition\n",
        "\n",
        "Complete the functions in the class below. Hints provided at appropriate places."
      ],
      "metadata": {
        "id": "kBtUcOVnJu-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegression:\n",
        "\n",
        "    # The __init__ is called when we make any object of our class. Here, you are to specify the default values for \n",
        "    # Learning Rate, Number of Iterations, Weights and Biases. It doesn't return anything.\n",
        "    # Hint: Google what a `self pointer` is and figure out how it can be used here.\n",
        "    def __init__(self, learning_rate=0.001, n_iters=1000,w=np.random.rand(X.shape[1]),b=1):\n",
        "      self.learning_rate=learning_rate\n",
        "      self.n_iters=n_iters\n",
        "      self.w=w\n",
        "      self.b=b  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #pass  # Uncomment this when you're done with this function\n",
        "\n",
        "\n",
        "    # The following function would be the heart of the model. This is where the training would happen. \n",
        "    # You're supposed to iterate and keep on updating the weights and biases according to the steps of Gradient Descent.\n",
        "    def fit(self, X, y):\n",
        "    \n",
        "      n=X.shape[0]\n",
        "      feature_count = X.shape[1]\n",
        "      for i in range(self.n_iters):\n",
        "        pred=np.matmul(X,self.w)+self.b\n",
        "        #error=np.sum(np.dot(pred-y,pred-y))/n\n",
        "        derivative = np.zeros(feature_count)\n",
        "        for j in range(feature_count):\n",
        "          test=np.dot(X[:,j],(y-pred))\n",
        "          derivative[j] = -2*np.sum(np.dot(X[:,j],(y-pred)),axis=None)/n\n",
        "        derivative_b=-2*np.sum(y-pred)/n \n",
        "        self.w=self.w-self.learning_rate*derivative\n",
        "        self.b=self.b-self.learning_rate*derivative_b\n",
        "      \n",
        "        \n",
        "\n",
        "        # Gradient Descent code goes here\n",
        "\n",
        "        #pass  # Uncomment this when you're done with this function\n",
        "        \n",
        "        \n",
        "    # This function will be called after our model has been trained and we are predicting on unseen data\n",
        "    # What is our prediction? Just return that\n",
        "    def predict(self, X):\n",
        "        # Code goes here\n",
        "        return np.matmul(X,self.w)+self.b\n",
        "       \n",
        "\n",
        "        #pass  # Uncomment this when you're done with this function"
      ],
      "metadata": {
        "id": "dGnFNPJx3I28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing, Training & Predictions"
      ],
      "metadata": {
        "id": "EvyInkTKPn7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we make an object of our custom class.\n",
        "regressor = LinearRegression() # You may pass the custom parameters or let the default values take it ahead\n",
        "\n",
        "# Call the fit method on the object to train (pass appropriate part of dataset)\n",
        "\n",
        "\n",
        "# Now, let's see our what our model predicts\n",
        "regressor.fit(X_train,y_train)\n",
        "predictions = regressor.predict(X_test) # pass appropriate part of dataset"
      ],
      "metadata": {
        "id": "nvItUpAkHTiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model   \n",
        "\n",
        "Return [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) & [R2 Score](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/coefficient-of-determination-r-squared.html#:~:text=%C2%AFy) from the functions below."
      ],
      "metadata": {
        "id": "tzK6cq8eRD4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "  return np.sum(np.dot(y_true-y_pred,y_true-y_pred))/y_true.shape[0]\n",
        "\n",
        "       # return the mean squared error\n",
        "       #pass  # Uncomment this when you're done with this function\n",
        "\n",
        "\n",
        "def r2_score(y_true, y_pred):\n",
        "  rss=np.sum(np.dot(y_true-y_pred,y_true-y_pred))\n",
        "  tss=np.sum(np.dot(y_true-np.mean(y_true),y_true-np.mean(y_true)))\n",
        "  return 1-rss/tss\n",
        "\n",
        "  \n",
        "      # return the r2 score\n",
        "      #pass  # Uncomment this when you're done with this function\n",
        "          \n",
        "\n",
        "mse = mean_squared_error(y_test,predictions) # Pass appropriate parts of dataset\n",
        "print(\"MSE:\", mse)\n",
        "\n",
        "accu = r2_score(y_test,predictions) # Pass appropriate parts of dataset\n",
        "print(\"Accuracy:\", accu)"
      ],
      "metadata": {
        "id": "WqkrvDzcRF5m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23cb6ae7-119b-4f78-ea4e-13d6821359c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 655.5771899128939\n",
            "Accuracy: 0.9380225238158303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iIzQNJm2iB9",
        "outputId": "8f7d257c-0a5b-451c-ddb1-88e25caeb077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38.07692227576615"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dUQREbqRJU67"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
